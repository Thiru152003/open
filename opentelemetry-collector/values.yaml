# ========================
# OpenTelemetry Collector
# Complete values.yaml
# ========================

# Core Configuration
enabled: true
mode: deployment  # deployment or daemonset
nameOverride: ""
fullnameOverride: ""
namespaceOverride: "otel-collector"

# Deployment Configuration
replicaCount: 2
revisionHistoryLimit: 10
annotations: {}
podAnnotations: {}
podLabels: {}

# Rollout Strategy (Critical Missing Field)
rollout:
  strategy: RollingUpdate  # RollingUpdate or Recreate
  rollingUpdate:
    maxSurge: 25%
    maxUnavailable: 25%

# Pod Disruption Budget
podDisruptionBudget:
  enabled: true
  minAvailable: 1
  maxUnavailable: ""

# Image Configuration
image:
  repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib
  tag: "0.96.0"
  pullPolicy: IfNotPresent
  imagePullSecrets: []

# Service Account
serviceAccount:
  create: true
  name: "otel-collector"
  annotations: {}
  automountServiceAccountToken: true

# Pod Security
podSecurityContext:
  fsGroup: 10001
  runAsUser: 10001
  runAsNonRoot: true

securityContext:
  capabilities:
    drop: ["ALL"]
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  seccompProfile:
    type: RuntimeDefault

# Resources
resources:
  limits:
    cpu: "1000m"
    memory: "2Gi"
  requests:
    cpu: "200m"
    memory: "1Gi"

# Probes
livenessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 15
  periodSeconds: 10
  timeoutSeconds: 1
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 1
  failureThreshold: 3

startupProbe:
  httpGet:
    path: /
    port: 13133
  failureThreshold: 30
  periodSeconds: 10

# Networking
hostNetwork: false
dnsPolicy: ClusterFirst
dnsConfig: {}
service:
  enabled: true
  type: ClusterIP
  annotations: {}
  clusterIP: ""
  externalTrafficPolicy: ""
  internalTrafficPolicy: "Cluster"
  loadBalancerIP: ""
  loadBalancerSourceRanges: []

# Ports Configuration
ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    protocol: TCP
    hostPort: null
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    protocol: TCP
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    protocol: TCP
  jaeger:
    enabled: true
    containerPort: 14250
    servicePort: 14250
    protocol: TCP
  jaeger-thrift-http:
    enabled: true
    containerPort: 14268
    servicePort: 14268
    protocol: TCP
  jaeger-thrift-compact:
    enabled: true
    containerPort: 6831
    servicePort: 6831
    protocol: UDP
  jaeger-thrift-binary:
    enabled: true
    containerPort: 6832
    servicePort: 6832
    protocol: UDP

# Environment Variables
extraEnvs:
  - name: MY_POD_IP
    valueFrom:
      fieldRef:
        fieldPath: status.podIP
  - name: OTEL_RESOURCE_ATTRIBUTES
    value: "k8s.cluster.name=my-cluster,k8s.namespace.name=$(NAMESPACE)"

# Scheduling
nodeSelector: {}
tolerations: []
affinity: {}
topologySpreadConstraints: []
priorityClassName: ""

# ConfigMap
configMap:
  create: true
  name: ""
  annotations: {}
  existingName: ""
  existingPath: ""

# Collector Configuration
config:
  extensions:
    health_check: {}
    memory_ballast:
      size_mib: 512
    # Add other extensions as needed

  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317"
        http:
          endpoint: "0.0.0.0:4318"
    prometheus:
      config:
        scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
              - targets: ["${env:MY_POD_IP}:8888"]
    jaeger:
      protocols:
        grpc:
          endpoint: "0.0.0.0:14250"
        thrift_http:
          endpoint: "0.0.0.0:14268"
        thrift_compact:
          endpoint: "0.0.0.0:6831"
        thrift_binary:
          endpoint: "0.0.0.0:6832"

  processors:
    batch:
      timeout: 5s
      send_batch_size: 10000
    memory_limiter:
      check_interval: 5s
      limit_percentage: 80
      spike_limit_percentage: 25
    resourcedetection:
      detectors: [env, system, gce, ecs, ec2, azure, eks, gke]
      timeout: 5s
      override: false

  exporters:
    logging:
      loglevel: debug
    otlp:
      endpoint: "otel-endpoint:4317"
      tls:
        insecure: false
    prometheus:
      endpoint: "0.0.0.0:8889"
    jaeger:
      endpoint: "jaeger-collector.observability.svc.cluster.local:14250"
      tls:
        insecure: true
      retry_on_failure:
        enabled: true
        initial_interval: 5s
        max_interval: 30s
        max_elapsed_time: 120s
      sending_queue:
        enabled: true
        num_consumers: 10
        queue_size: 5000

  service:
    pipelines:
      traces:
        receivers: [otlp, jaeger]
        processors: [memory_limiter, resourcedetection, batch]
        exporters: [logging, jaeger]
      metrics:
        receivers: [otlp, prometheus]
        processors: [memory_limiter, resourcedetection, batch]
        exporters: [logging, prometheus]
      logs:
        receivers: [otlp]
        processors: [memory_limiter, resourcedetection, batch]
        exporters: [logging]
    extensions: [health_check, memory_ballast]

# Presets Configuration
presets:
  logsCollection:
    enabled: true
    includeCollectorLogs: false
    storeCheckpoints: false
    maxRecombineLogSize: 102400
  hostMetrics:
    enabled: true
  kubeletMetrics:
    enabled: true
  kubernetesAttributes:
    enabled: true
    extractAllPodLabels: false
    extractAllPodAnnotations: false
  kubernetesEvents:
    enabled: false
  clusterMetrics:
    enabled: false

# Monitoring
podMonitor:
  enabled: false
  additionalLabels: {}
  interval: ""
  scrapeTimeout: ""
  relabelings: []
  metricRelabelings: []

serviceMonitor:
  enabled: false
  additionalLabels: {}
  interval: ""
  scrapeTimeout: ""
  relabelings: []
  metricRelabelings: []

prometheusRule:
  enabled: false
  additionalLabels: {}
  rules: []

# Autoscaling
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80
  behavior: {}

# Additional Features
useGOMEMLIMIT: true
shareProcessNamespace: false
lifecycle: {}
terminationGracePeriodSeconds: 30
extraVolumeMounts: []
extraVolumes: []
extraContainers: []
extraInitContainers: []
